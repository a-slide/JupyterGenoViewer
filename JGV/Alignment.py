# -*- coding: utf-8 -*-

"""
  Alignment.py
  JGV is a Python3 package for an embed genomic viewer in Jupyter notebook. Do not import the package in a
  non-interactive environment

  Copyright 2016 Adrien Leger <aleg@ebi.ac.ul>
  [Github](https://github.com/a-slide)

  This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public
  License as published by the Free Software Foundation; either version 3 of the License, or(at your option) any later
  version

  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied
  warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
  (http://www.gnu.org/licenses/gpl-3.0.html).

  You should have received a copy of the GNU General Public License along with this program; if not, write to the Free
  Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
"""

# Strandard library imports
from collections import OrderedDict, Counter
from os import access, R_OK
import gzip

# Third party import
import pysam
import pandas as pd
import numpy as np

# Local lib import
from JGV_helper_fun import *
from JGV_helper_fun import jprint as print

#~~~~~~~CLASS~~~~~~~#

class Alignment(object):
    """
    Parse data and compute the base resolution coverage from a file containing aligned reads in BAM, SAM or BED format.
    Can return the coverage for a given interval
    """

    #~~~~~~~FUNDAMENTAL METHODS~~~~~~~#

    def __init__ (self, fp, name=None, verbose=False, min_coverage=5, ref_list=[], output_bed=False):
        """
         * fp
             A standard BAM or SAM (http://samtools.sourceforge.net/SAM1.pdf) containing aligned reads and a standard
             header. The files do not need to be sorted or indexed.
             One can also use a 6 fields bed (chrom, chromStart, chromEnd, name, score, strand) file with a hastaged
             commented header listing the reference sequences id and length, similar to the format generated by the
             output_bed option (Much faster than from a Bam/Sam file, can be gzipped).
             http://www.ensembl.org/info/website/upload/bed.html
        *  name
            Name of the data file that will be used as track name for plotting. If not given, will be deduced from fp
            file name  [ DEFAULT: None ]
        * verbose
            If True, will print more information during initialisation and calls of all the object methods
            [ DEFAULT: False ]
        * min_coverage
            Minimal coverage to compute the data. If less, the coverage will be considered null. Not used for
            if fp is a bed coverage file [ DEFAULT: 5 ]
        * ref_list
            list of reference sequence id to select from the data file, by default all, Not used for if fp is a bed
            coverage file [ DEFAULT: [] ]
        * output_bed
            If True will be write a 6 columns compressed bed file containing the coverage values for + and - strand
            excluding positions with coverage lesser than min_coverage.the option will apply only is the input file is
            BAM or SAM. The file starts with a header consisting of a list of the ID of the reference sequences and
            their length [ DEFAULT: False ]. Example:
              #chr20	64444167
              #chr21	46709983
              chr20	276516	276516	pos1	5	+
              chr20	276517	276517	pos2	5	+
        """
        # Verify that the file is readable
        assert access(fp, R_OK), "{} is not readable".format(fp)

        #Save self variable
        self.fp = fp
        self.name = name if name else file_basename(fp)
        self.verbose = verbose
        self.ext = extensions(fp)[0]
        self.nbases = 0

        if self.ext in ["bam","sam"]:
            if self.verbose: print("Compute coverage from bam/sam file ", self.fp)
            self.d = self._bam_parser(fp, min_coverage, ref_list)
            if output_bed:
                #assert access(output_file, W_OK), "{} is not writable".format(fp)
                outfp = "{}/{}.bed.gz".format(dir_path(fp), file_basename(fp))
                if self.verbose: print("Write coverage data in file ", outfp)
                self._write_coverage_file (outfp)
                self.outfp=outfp

        elif self.ext == "bed":
            if self.verbose: print("Extract coverage from bed file", self.fp)
            self.d = self._bed_parser(fp)
        else:
            msg = "The file is not in SAM/BAM/BED format. Please provide a correctly formated file"
            raise ValueError(msg)

    def __str__(self):
        """readable description of the object"""
        msg = "{} instance\n".format(self.__class__.__name__)
        # list all values in object dict in alphabetical order
        for k,v in OrderedDict(sorted(self.__dict__.items(), key=lambda t: t[0])).items():
            if k == "d":
                for refid, refval in v.items():
                    msg+="\t{}\tlength:{}\tnbases:{}\n".format(refid, refval["length"], refval["nbases"])
            else:
                msg+="\t{}\t{}\n".format(k, v)
        return (msg)

    def __repr__ (self):
        return ("{}-{}".format(self.__class__.__name__, self.name))

    #~~~~~~~PROPERTY METHODS~~~~~~~#

    @property
    def refid_list(self):
        """List of unique reference sequence ids found"""
        return list(self.d.keys())

    @property
    def refid_len(self):
        """List of unique reference sequence ids found associated with their length"""
        s = pd.Series(name="length")
        for refid, refval in self.d.items():
            s.loc[refid] = refval["length"]
        return s.sort_values(ascending=False)

    @property
    def refid_nbases(self):
        """List of unique reference sequence ids found associated with their base coverage"""
        s = pd.Series(name="nbases")
        for refid, refval in self.d.items():
            s.loc[refid] = refval["nbases"]
        return s.sort_values(ascending=False)

    @property
    def refid_count (self):
        """
        Return a dataframe containing 2 columns:
          * nbase = Total base coverage per refid
          * bpkm = Base coverage normalized by length of the refid and the overal base coverage for all sequences
        """
        df = pd.DataFrame(columns=["nbases", "bpkm"])
        for refid, refval in self.d.items():
            df.loc[refid] = [
                refval["nbases"],
                refval["nbases"]/(refval["length"]/1000)/(self.nbases/1000000)]
        return df

    #~~~~~~~PRIVATE METHODS~~~~~~~#

    def _bam_parser(self, fp, min_coverage=5, ref_list= []):
        """Parse a sam or bam formated file"""
        d = OrderedDict()
        with pysam.AlignmentFile(fp) as bam:
            # Save meta-information for references sequences based on the bam header
            if self.verbose: print("\tExtract meta data from header")
            for refid, length in zip(bam.references, bam.lengths):
                if not ref_list or refid in ref_list:
                    d[refid] = {"length":length, "nbases":0, "+":Counter(), "-":Counter()}
            # Compute the genomic coverage for each reads
            if self.verbose: print("\tTally coverage for each base")
            for line in bam:
                refid = line.reference_name
                if refid in d:
                    strand = "-" if line.is_reverse else "+"
                    for position in line.get_reference_positions():
                        d[refid][strand][position] += 1
        # Remove base with coverage below threshold and transform in Pandas Series
        if self.verbose: print("\tFilter and sort the coverage results by position")
        for refid in d.keys():
            for strand in ["+","-"]:
                s = OrderedDict()
                for position, coverage in d[refid][strand].items():
                    if coverage >= min_coverage:
                        s[position] = coverage
                        self.nbases += coverage
                        d[refid]["nbases"] += coverage
                d[refid][strand] = pd.Series(s)#, dtype="int64")
                d[refid][strand].sort_index(inplace=True)
        return d

    def _bed_parser (self, fp):
        """Extract data from a coverage bad file"""
        d = OrderedDict()

        # File handling for both uncompressed or compressed fasta file
        if fp.endswith(".gz"):
            open_fun, open_mode = gzip.open, "rt"
        else:
            open_fun, open_mode = open, "r"
        # Parse fasta file refid and count the length of each sequence
        with open_fun(fp, open_mode) as fin:
            if self.verbose: print("\tExtract meta data from header and save base coverage data")
            for line in fin:
                # Extract metadata from header lines
                if line.startswith("#"):
                    sl = line[1:-1].split("\t")
                    refid = sl[0]
                    length = int(sl[1])
                    d[refid] = {"length":length, "nbases":0, "+":OrderedDict(), "-":OrderedDict()}
                # Extract coverage value for each position
                else:
                    sl = line[0:-1].split("\t")
                    refid = sl[0]
                    position = int(sl[1])
                    coverage = int(sl[4])
                    strand = sl[5]
                    d[refid][strand][position] = coverage
                    d[refid]["nbases"] += coverage
                    self.nbases += coverage
            # Cast into Pandas Series and sort by coordinates
            if self.verbose: print("\tSort the coverage results by position")
            for refid in d.keys():
                for strand in ["+","-"]:
                    d[refid][strand] = pd.Series(d[refid][strand], dtype="int64")
                    d[refid][strand].sort_index(inplace=True)

        return d

    def _write_coverage_file (self, outfp, buf_size=8192 ):
        """Bufferized writer for the coverage bed file"""
        with gopen (outfp, "wt") as out:
            # Write header containing chromosome information
            for refid, refval in self.d.items():
                out.write("#{}\t{}\n".format(refid, refval["length"]))
            # Bufferized writing of lines
            i = 0
            str_buf = ""
            for refid, refval in self.d.items():
                for strand in ["+","-"]:
                    for position, coverage in refval[strand].items():
                        i+=1
                        str_buf += "{0}\t{1}\t{1}\tpos{2}\t{3}\t{4}\n".format(refid, position, i, coverage, strand)
                        if i%buf_size == 0:
                            out.write(str_buf)
                            str_buf = ""
            # Empty the rest of the buffer
            out.write(str_buf)
            str_buf = ""

    #~~~~~~~PUBLIC METHODS~~~~~~~#

    def interval_coverage (self, refid, start, end, bins=500, bin_repr_fun = "max"):
        """
        Parse the alignment file for a given refid and interval. The interval is splited in a number of windows equal to
        bins, for which the coverage in computed. The method return a dataframe containing the starting positions of
        the windows and the coverage for the + and - strands. If the refid or the coordinates are invalid a zero filled
        dataframe will be returned.
        * seqid
            Name of the sequence from the original fasta file to display
        * start
            Start of the window to display. The coordinate is not verified, if outside of the range it will
            return empty bins
        * end
            End of the window to display. The coordinate is not verified, if outside of the range it will
            return empty bins
       * bins
            Number of alignment count bins to divide the displayed window. Low number will result in low resolution
            high value could result in a long ploting time. The valur is automatically adjust is lower than base
            resolution, ie if the requested interval is lower than the number of bins [ DEFAULT: 500 ]
        * bin_repr_fun
            Function to represent each bin ("max", "mean" and "sum") [ DEFAULT: "max" ]
        """
        if self.verbose: print ("Compute coverage from the windows: {}:{}-{}".format(refid, start, end))
        df = pd.DataFrame(columns=["+", "-"], dtype=int)

        # Adjust number of bins and calculate step
        if bins > end-start:
            bins = end-start
            if self.verbose: print ("\tAuto adjust the number of bins to match the interval: {}".format(bins))
        step = (end-start)/bins
        if self.verbose: print ("\tDefine size of each bin: {}".format(step))

        # If refid is not in the self refid-list
        if not refid in self.refid_list:
            if self.verbose: print ("\tThe reference {} is not in the list of references with alignment".format(refid))
            for i in np.arange (start, end, step):
                for strand in ["+","-"]:
                    df.loc[int(i), strand] =  0
            return df

        # Select positions windows and get maximun
        if self.verbose: print ("\tCompute coverage...")
        for i in np.arange (start, end, step):
            winstart = int(i)
            winend = int(i+step)
            for strand in ["+","-"]:
                l = self.d[refid][strand][(self.d[refid][strand].index >= winstart)&(self.d[refid][strand].index < winend)]
                if l.empty:
                    df.loc[winstart, strand] =  0
                elif bin_repr_fun == "max":
                    df.loc[winstart, strand] = l.max()
                elif bin_repr_fun == "sum":
                    df.loc[winstart, strand] = l.sum()
                elif bin_repr_fun == "mean":
                    df.loc[winstart, strand] = l.sum()/step
        if self.verbose:
            if df["+"].sum() + df["-"].sum() == 0:
                print("\tNull coverage for both strands in the requested interval")
            elif df["+"].sum() == 0:
                print("\tNull coverage for the positive strand in the requested interval")
            elif df["-"].sum() == 0:
                print("\tNull coverage for the negative strand in the requested interval")
        return df
